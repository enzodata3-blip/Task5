{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRNet Topological Optimization - Testing Notebook\n",
    "\n",
    "This notebook tests all components of the topological optimization system.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- PyTorch 2.0+\n",
    "- TDA libraries (gudhi, ripser, persim)\n",
    "\n",
    "Run this notebook to verify everything works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and check versions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all required packages\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'numpy',\n",
    "    'scipy',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'sklearn',\n",
    "    'ripser',\n",
    "    'persim',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "print(\"Checking required packages...\\n\")\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        mod = importlib.import_module(package)\n",
    "        version = getattr(mod, '__version__', 'unknown')\n",
    "        print(f\"✓ {package:15s} {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"✗ {package:15s} NOT FOUND\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n⚠️  Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"\\nTo install missing packages, run:\")\n",
    "    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n✓ All required packages are installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add hrnet_base to path\n",
    "project_root = Path(os.getcwd())\n",
    "hrnet_lib_path = project_root / 'hrnet_base' / 'lib'\n",
    "sys.path.insert(0, str(hrnet_lib_path))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"HRNet lib path: {hrnet_lib_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main project modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import custom modules\n",
    "from topology_analyzer import TopologicalAnalyzer, TopologyAwareTraining\n",
    "from train_enhanced import HRNetCIFAR\n",
    "\n",
    "print(\"✓ All project modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Topological Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic topological analysis on synthetic data\n",
    "print(\"Testing TopologicalAnalyzer...\\n\")\n",
    "\n",
    "# Create synthetic data: two clusters\n",
    "np.random.seed(42)\n",
    "cluster1 = np.random.randn(50, 10) + np.array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "cluster2 = np.random.randn(50, 10) - np.array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "data = np.vstack([cluster1, cluster2])\n",
    "\n",
    "print(f\"Test data shape: {data.shape}\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = TopologicalAnalyzer(max_dimension=1, distance_threshold=5.0)\n",
    "print(\"✓ TopologicalAnalyzer initialized\")\n",
    "\n",
    "# Compute persistence diagram\n",
    "stats = analyzer.compute_persistence_diagram(data, label='test')\n",
    "\n",
    "if stats:\n",
    "    print(\"\\n✓ Persistence diagram computed successfully!\")\n",
    "    print(f\"  Betti numbers: {stats['betti_numbers']}\")\n",
    "    print(f\"  Persistence entropy: {stats['persistence_entropy']:.4f}\")\n",
    "    print(f\"  Number of H_0 features: {len(stats['diagrams'][0])}\")\n",
    "    print(f\"  Number of H_1 features: {len(stats['diagrams'][1]) if len(stats['diagrams']) > 1 else 0}\")\n",
    "else:\n",
    "    print(\"✗ Failed to compute persistence diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bottleneck distance computation\n",
    "print(\"Testing bottleneck distance computation...\\n\")\n",
    "\n",
    "# Create two similar clusters\n",
    "data1 = np.random.randn(50, 10)\n",
    "data2 = np.random.randn(50, 10) + 0.1  # Slight shift\n",
    "\n",
    "distance = analyzer.compute_bottleneck_distance(data1, data2, dimension=0)\n",
    "print(f\"Bottleneck distance (H_0): {distance:.4f}\")\n",
    "\n",
    "if distance < float('inf'):\n",
    "    print(\"✓ Bottleneck distance computed successfully!\")\n",
    "    print(f\"  Distance is {'small' if distance < 1.0 else 'large'} (expected: small for similar data)\")\n",
    "else:\n",
    "    print(\"✗ Failed to compute bottleneck distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize persistence diagram\n",
    "print(\"Testing persistence diagram visualization...\\n\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('test_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Visualize\n",
    "analyzer.visualize_persistence_diagram(\n",
    "    data,\n",
    "    save_path=output_dir / 'test_persistence.png'\n",
    ")\n",
    "\n",
    "print(\"✓ Persistence diagram saved to test_output/test_persistence.png\")\n",
    "\n",
    "# Display inline\n",
    "from IPython.display import Image, display\n",
    "if (output_dir / 'test_persistence.png').exists():\n",
    "    display(Image(filename=str(output_dir / 'test_persistence.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test HRNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model instantiation\n",
    "print(\"Testing HRNetCIFAR model...\\n\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = HRNetCIFAR(num_classes=10, width=18)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"✓ Model created successfully\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"Testing forward pass...\\n\")\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, 3, 32, 32).to(device)\n",
    "\n",
    "# Forward pass without features\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"✓ Standard forward pass successful\")\n",
    "\n",
    "# Forward pass with features\n",
    "output, features = model(dummy_input, return_features=True)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"✓ Forward pass with feature extraction successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CIFAR-10 data loading\n",
    "print(\"Testing CIFAR-10 data loading...\\n\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Download a small subset for testing\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"✓ CIFAR-10 test dataset loaded\")\n",
    "print(f\"  Number of samples: {len(test_dataset)}\")\n",
    "print(f\"  Classes: {test_dataset.classes}\")\n",
    "\n",
    "# Create data loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loader created\")\n",
    "print(f\"  Number of batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "print(\"Visualizing sample images...\\n\")\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(img):\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    return img * std + mean\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = denormalize(images[i])\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(test_dataset.classes[labels[i]], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_output/sample_images.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Sample images visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Topology-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test topology-aware training components\n",
    "print(\"Testing TopologyAwareTraining...\\n\")\n",
    "\n",
    "topology_trainer = TopologyAwareTraining(topology_weight=0.01)\n",
    "print(\"✓ TopologyAwareTraining initialized\")\n",
    "\n",
    "# Test loss computation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get a batch\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output, features = model(images, return_features=True)\n",
    "\n",
    "# Compute combined loss\n",
    "combined_loss, loss_stats = topology_trainer.compute_combined_loss(\n",
    "    output, labels, features, criterion\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Combined loss computed successfully!\")\n",
    "print(f\"  Base loss: {loss_stats['base_loss']:.4f}\")\n",
    "print(f\"  Topological loss: {loss_stats['topo_loss']:.4f}\")\n",
    "print(f\"  Total loss: {loss_stats['total_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Feature Extraction and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a small subset\n",
    "print(\"Extracting features from test data...\\n\")\n",
    "\n",
    "model.eval()\n",
    "features_list = []\n",
    "labels_list = []\n",
    "max_batches = 10  # Use 10 batches for quick testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(tqdm(test_loader, desc='Extracting')):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "\n",
    "        images = images.to(device)\n",
    "        _, features = model(images, return_features=True)\n",
    "\n",
    "        features_list.append(features.cpu().numpy())\n",
    "        labels_list.extend(labels.numpy())\n",
    "\n",
    "# Concatenate\n",
    "all_features = np.concatenate(features_list, axis=0)\n",
    "all_labels = np.array(labels_list)\n",
    "\n",
    "print(f\"\\n✓ Features extracted\")\n",
    "print(f\"  Shape: {all_features.shape}\")\n",
    "print(f\"  Number of samples: {len(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize features by class\n",
    "print(\"Organizing features by class...\\n\")\n",
    "\n",
    "features_by_class = {}\n",
    "for class_id in range(10):\n",
    "    mask = all_labels == class_id\n",
    "    features_by_class[class_id] = all_features[mask]\n",
    "    print(f\"  Class {class_id} ({test_dataset.classes[class_id]:12s}): {np.sum(mask)} samples\")\n",
    "\n",
    "print(\"\\n✓ Features organized by class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute topological features for each class\n",
    "print(\"Computing topological features per class...\\n\")\n",
    "\n",
    "analyzer_test = TopologicalAnalyzer(max_dimension=1, distance_threshold=3.0)\n",
    "class_topology = {}\n",
    "\n",
    "for class_id in tqdm(range(10), desc='Analyzing classes'):\n",
    "    features = features_by_class[class_id]\n",
    "\n",
    "    if len(features) > 10:  # Need minimum samples\n",
    "        # Sample if too many\n",
    "        if len(features) > 100:\n",
    "            indices = np.random.choice(len(features), 100, replace=False)\n",
    "            features = features[indices]\n",
    "\n",
    "        stats = analyzer_test.compute_persistence_diagram(\n",
    "            features,\n",
    "            label=f'class_{class_id}'\n",
    "        )\n",
    "\n",
    "        if stats:\n",
    "            class_topology[class_id] = stats\n",
    "\n",
    "print(f\"\\n✓ Topological analysis complete for {len(class_topology)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topology statistics\n",
    "print(\"\\nTopological Statistics per Class:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Class':<12} {'Name':<15} {'Betti-0':<10} {'Betti-1':<10} {'Entropy':<12}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for class_id in sorted(class_topology.keys()):\n",
    "    stats = class_topology[class_id]\n",
    "    name = test_dataset.classes[class_id]\n",
    "    betti = stats['betti_numbers']\n",
    "    betti_0 = betti[0] if len(betti) > 0 else 0\n",
    "    betti_1 = betti[1] if len(betti) > 1 else 0\n",
    "    entropy = stats['persistence_entropy']\n",
    "\n",
    "    print(f\"{class_id:<12} {name:<15} {betti_0:<10} {betti_1:<10} {entropy:<12.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Bottleneck Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inter-class bottleneck distances\n",
    "print(\"Computing inter-class bottleneck distances...\\n\")\n",
    "\n",
    "num_classes = 10\n",
    "distance_matrix = np.zeros((num_classes, num_classes))\n",
    "\n",
    "for i in tqdm(range(num_classes), desc='Computing distances'):\n",
    "    for j in range(i+1, num_classes):\n",
    "        if i in features_by_class and j in features_by_class:\n",
    "            feat_i = features_by_class[i]\n",
    "            feat_j = features_by_class[j]\n",
    "\n",
    "            # Sample if needed\n",
    "            if len(feat_i) > 100:\n",
    "                feat_i = feat_i[np.random.choice(len(feat_i), 100, replace=False)]\n",
    "            if len(feat_j) > 100:\n",
    "                feat_j = feat_j[np.random.choice(len(feat_j), 100, replace=False)]\n",
    "\n",
    "            distance = analyzer_test.compute_bottleneck_distance(\n",
    "                feat_i, feat_j, dimension=0\n",
    "            )\n",
    "\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance\n",
    "\n",
    "print(\"\\n✓ Distance matrix computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distance matrix\n",
    "print(\"Visualizing bottleneck distance matrix...\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "    distance_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=test_dataset.classes,\n",
    "    yticklabels=test_dataset.classes,\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Bottleneck Distance'}\n",
    ")\n",
    "\n",
    "plt.title('Inter-Class Bottleneck Distance Matrix\\n(Untrained Model)',\n",
    "          fontsize=14, pad=20)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.ylabel('Class', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_output/distance_matrix_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Distance matrix visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ All components tested successfully!\\n\")\n",
    "\n",
    "print(\"Components verified:\")\n",
    "print(\"  1. ✓ TopologicalAnalyzer - persistent homology computation\")\n",
    "print(\"  2. ✓ Bottleneck distance - topological similarity measurement\")\n",
    "print(\"  3. ✓ HRNetCIFAR model - forward pass and feature extraction\")\n",
    "print(\"  4. ✓ Data loading - CIFAR-10 dataset\")\n",
    "print(\"  5. ✓ TopologyAwareTraining - combined loss computation\")\n",
    "print(\"  6. ✓ Feature analysis - per-class topology\")\n",
    "print(\"  7. ✓ Distance matrix - inter-class comparison\")\n",
    "print(\"  8. ✓ Visualizations - persistence diagrams and heatmaps\")\n",
    "\n",
    "print(\"\\nOutput files created in test_output/:\")\n",
    "output_files = list(Path('test_output').glob('*'))\n",
    "for f in sorted(output_files):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR TRAINING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo start training with topological optimization:\")\n",
    "print(\"  python train_enhanced.py --dataset cifar10 --topology-weight 0.01\")\n",
    "print(\"\\nOr run the quick start script:\")\n",
    "print(\"  ./quick_start.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Quick Training Test (1 Epoch)\n",
    "\n",
    "Run this cell to test a complete training iteration (takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test one epoch of training\n",
    "print(\"Testing one training epoch...\\n\")\n",
    "\n",
    "# Setup\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "topology_trainer = TopologyAwareTraining(topology_weight=0.01)\n",
    "\n",
    "# Train on a few batches\n",
    "num_batches = 10\n",
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, (images, labels) in enumerate(tqdm(test_loader, desc='Training')):\n",
    "    if i >= num_batches:\n",
    "        break\n",
    "\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, features = model(images, return_features=True)\n",
    "\n",
    "    # Compute loss with topology\n",
    "    loss, loss_stats = topology_trainer.compute_combined_loss(\n",
    "        output, labels, features, criterion\n",
    "    )\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Statistics\n",
    "    running_loss += loss.item()\n",
    "    _, predicted = output.max(1)\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "# Results\n",
    "avg_loss = running_loss / num_batches\n",
    "accuracy = 100. * correct / total\n",
    "\n",
    "print(f\"\\n✓ Training test complete!\")\n",
    "print(f\"  Average loss: {avg_loss:.4f}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"\\n  (Note: Low accuracy is expected for untrained model)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
