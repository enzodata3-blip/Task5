{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Test - HRNet Topological Optimization\n",
    "\n",
    "## Purpose\n",
    "This notebook provides a **fast verification** (2-3 minutes) that everything is working.\n",
    "\n",
    "For comprehensive testing, use `test_topology_optimization.ipynb`\n",
    "\n",
    "## Instructions\n",
    "1. Run all cells (Cell → Run All)\n",
    "2. Check for ✓ checkmarks\n",
    "3. If you see \"✓ ALL SYSTEMS GO!\", everything works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK VERIFICATION TEST\n",
      "================================================================================\n",
      "\n",
      "Python: 3.13.5\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUICK VERIFICATION TEST\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "\n",
    "# Track test results\n",
    "tests_passed = 0\n",
    "tests_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing core dependencies...\n",
      "✓ PyTorch: 2.10.0\n",
      "✓ CUDA available: False\n",
      "✓ NumPy: 2.1.3\n",
      "✓ ripser: Available\n",
      "✓ persim: Available\n",
      "\n",
      "✓ PASS: All core dependencies imported successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing core dependencies...\")\n",
    "tests_total += 1\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import numpy as np\n",
    "    from ripser import ripser\n",
    "    from persim import bottleneck\n",
    "    \n",
    "    print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "    print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"✓ NumPy: {np.__version__}\")\n",
    "    print(f\"✓ ripser: Available\")\n",
    "    print(f\"✓ persim: Available\")\n",
    "    \n",
    "    tests_passed += 1\n",
    "    print(\"\\n✓ PASS: All core dependencies imported successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ FAIL: {e}\\n\")\n",
    "    print(\"Fix: pip install torch torchvision numpy ripser persim\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing custom modules...\n",
      "\n",
      "✗ FAIL: No module named 'tensorboard'\n",
      "\n",
      "\n",
      "Fix: Ensure you're in the model_b directory\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/sq/89m0gwkx2fn379dwl7ckt74r0000gn/T/ipykernel_3264/4258145903.py\", line 16, in <module>\n",
      "    from train_enhanced import HRNetCIFAR\n",
      "  File \"/Users/apple/Documents/Task5/model_b/train_enhanced.py\", line 21, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/torch/utils/tensorboard/__init__.py\", line 1, in <module>\n",
      "    import tensorboard\n",
      "ModuleNotFoundError: No module named 'tensorboard'\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing custom modules...\")\n",
    "tests_total += 1\n",
    "\n",
    "try:\n",
    "    # Add paths\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    project_root = Path(os.getcwd())\n",
    "    hrnet_lib_path = project_root / 'hrnet_base' / 'lib'\n",
    "    if hrnet_lib_path.exists():\n",
    "        sys.path.insert(0, str(hrnet_lib_path))\n",
    "    \n",
    "    # Import custom modules\n",
    "    from topology_analyzer import TopologicalAnalyzer, TopologyAwareTraining\n",
    "    from train_enhanced import HRNetCIFAR\n",
    "    \n",
    "    print(\"✓ topology_analyzer imported\")\n",
    "    print(\"✓ train_enhanced imported\")\n",
    "    \n",
    "    tests_passed += 1\n",
    "    print(\"\\n✓ PASS: All custom modules imported successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ FAIL: {e}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nFix: Ensure you're in the model_b directory\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing topological analysis...\n",
      "✓ TopologicalAnalyzer initialized\n",
      "✓ Persistence computed: Betti = [30, 13]\n",
      "✓ Entropy = 5.946\n",
      "✓ Bottleneck distance = 0.722\n",
      "\n",
      "✓ PASS: Topological analysis working correctly\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/persim/bottleneck.py:55: UserWarning: dgm1 has points with non-finite death times;ignoring those points\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.13/site-packages/persim/bottleneck.py:64: UserWarning: dgm2 has points with non-finite death times;ignoring those points\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing topological analysis...\")\n",
    "tests_total += 1\n",
    "\n",
    "try:\n",
    "    # Create test data\n",
    "    np.random.seed(42)\n",
    "    test_data = np.random.randn(30, 10)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = TopologicalAnalyzer(max_dimension=1, distance_threshold=5.0)\n",
    "    print(\"✓ TopologicalAnalyzer initialized\")\n",
    "    \n",
    "    # Compute persistence\n",
    "    stats = analyzer.compute_persistence_diagram(test_data)\n",
    "    \n",
    "    if stats and 'betti_numbers' in stats:\n",
    "        print(f\"✓ Persistence computed: Betti = {stats['betti_numbers']}\")\n",
    "        print(f\"✓ Entropy = {stats['persistence_entropy']:.3f}\")\n",
    "        \n",
    "        # Test bottleneck distance\n",
    "        test_data2 = np.random.randn(30, 10)\n",
    "        distance = analyzer.compute_bottleneck_distance(test_data, test_data2, dimension=0)\n",
    "        \n",
    "        if distance < float('inf'):\n",
    "            print(f\"✓ Bottleneck distance = {distance:.3f}\")\n",
    "            tests_passed += 1\n",
    "            print(\"\\n✓ PASS: Topological analysis working correctly\\n\")\n",
    "        else:\n",
    "            print(\"\\n⚠ WARNING: Bottleneck distance computation returned inf\\n\")\n",
    "    else:\n",
    "        print(\"\\n⚠ WARNING: Persistence diagram computation returned empty\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ FAIL: {e}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model creation...\n",
      "\n",
      "✗ FAIL: name 'HRNetCIFAR' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/sq/89m0gwkx2fn379dwl7ckt74r0000gn/T/ipykernel_3264/395184058.py\", line 7, in <module>\n",
      "    model = HRNetCIFAR(num_classes=10, width=18)\n",
      "            ^^^^^^^^^^\n",
      "NameError: name 'HRNetCIFAR' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing model creation...\")\n",
    "tests_total += 1\n",
    "\n",
    "try:\n",
    "    # Create model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HRNetCIFAR(num_classes=10, width=18)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✓ Model created with {num_params:,} parameters\")\n",
    "    print(f\"✓ Device: {device}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "        print(f\"✓ Standard forward: {output.shape}\")\n",
    "        \n",
    "        output, features = model(dummy_input, return_features=True)\n",
    "        print(f\"✓ With features: output {output.shape}, features {features.shape}\")\n",
    "    \n",
    "    tests_passed += 1\n",
    "    print(\"\\n✓ PASS: Model creation and forward pass working\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ FAIL: {e}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Loss (Topology + Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing topology-aware loss...\n",
      "\n",
      "✗ FAIL: name 'model' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/sq/89m0gwkx2fn379dwl7ckt74r0000gn/T/ipykernel_3264/3097574781.py\", line 17, in <module>\n",
      "    output, features = model(dummy_input, return_features=True)\n",
      "                       ^^^^^\n",
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing topology-aware loss...\")\n",
    "tests_total += 1\n",
    "\n",
    "try:\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Create components\n",
    "    topology_trainer = TopologyAwareTraining(topology_weight=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create dummy data\n",
    "    dummy_input = torch.randn(4, 3, 32, 32).to(device)\n",
    "    dummy_labels = torch.randint(0, 10, (4,)).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output, features = model(dummy_input, return_features=True)\n",
    "    \n",
    "    # Compute combined loss\n",
    "    loss, stats = topology_trainer.compute_combined_loss(\n",
    "        output, dummy_labels, features, criterion\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Base loss: {stats['base_loss']:.4f}\")\n",
    "    print(f\"✓ Topo loss: {stats['topo_loss']:.4f}\")\n",
    "    print(f\"✓ Total loss: {stats['total_loss']:.4f}\")\n",
    "    \n",
    "    tests_passed += 1\n",
    "    print(\"\\n✓ PASS: Topology-aware loss computation working\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ FAIL: {e}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Tests passed: {tests_passed}/{tests_total}\")\n",
    "print()\n",
    "\n",
    "if tests_passed == tests_total:\n",
    "    print(\"\"\"✓ ✓ ✓ ALL SYSTEMS GO! ✓ ✓ ✓\n",
    "\n",
    "Your installation is working perfectly!\n",
    "\n",
    "Next steps:\n",
    "1. Run the comprehensive test: test_topology_optimization.ipynb\n",
    "2. Or start training: python train_enhanced.py --dataset cifar10\n",
    "3. Or run quick experiment: ./quick_start.sh\n",
    "\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"⚠ {tests_total - tests_passed} test(s) failed\n",
    "\n",
    "Please review the errors above and:\n",
    "1. Check that all dependencies are installed: pip install -r requirements_enhanced.txt\n",
    "2. Run: python check_environment.py\n",
    "3. See INSTALL.md for troubleshooting\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Visual Test\n",
    "\n",
    "Run this cell to see a persistence diagram (visual confirmation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create test data with clear structure\n",
    "np.random.seed(42)\n",
    "cluster1 = np.random.randn(25, 5) + np.array([3, 0, 0, 0, 0])\n",
    "cluster2 = np.random.randn(25, 5) - np.array([3, 0, 0, 0, 0])\n",
    "data = np.vstack([cluster1, cluster2])\n",
    "\n",
    "# Compute topology\n",
    "analyzer_viz = TopologicalAnalyzer(max_dimension=1, distance_threshold=10.0)\n",
    "stats_viz = analyzer_viz.compute_persistence_diagram(data)\n",
    "\n",
    "if stats_viz:\n",
    "    diagrams = stats_viz['diagrams']\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # H_0 (components)\n",
    "    dgm0 = diagrams[0]\n",
    "    axes[0].scatter(dgm0[:, 0], dgm0[:, 1], alpha=0.6, s=50)\n",
    "    lims = [0, max(dgm0.max(), 1)]\n",
    "    axes[0].plot(lims, lims, 'k--', alpha=0.3)\n",
    "    axes[0].set_xlabel('Birth', fontsize=12)\n",
    "    axes[0].set_ylabel('Death', fontsize=12)\n",
    "    axes[0].set_title('H₀ Persistence (Components)', fontsize=13)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # H_1 (loops)\n",
    "    if len(diagrams) > 1:\n",
    "        dgm1 = diagrams[1]\n",
    "        if len(dgm1) > 0:\n",
    "            axes[1].scatter(dgm1[:, 0], dgm1[:, 1], alpha=0.6, s=50, color='orange')\n",
    "            lims = [0, max(dgm1.max(), 1)]\n",
    "            axes[1].plot(lims, lims, 'k--', alpha=0.3)\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'No H₁ features', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    axes[1].set_xlabel('Birth', fontsize=12)\n",
    "    axes[1].set_ylabel('Death', fontsize=12)\n",
    "    axes[1].set_title('H₁ Persistence (Loops)', fontsize=13)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Visual test complete!\")\n",
    "    print(f\"  Betti-0: {stats_viz['betti_numbers'][0]} (should be ~2 for two clusters)\")\n",
    "    print(f\"  Entropy: {stats_viz['persistence_entropy']:.3f}\")\n",
    "else:\n",
    "    print(\"Could not generate visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
